{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from contextlib import suppress\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage.external.tifffile import imread, imsave\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import applications\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the [UC Merced Land Use dataset](http://vision.ucmerced.edu/datasets/landuse.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image files from the zipped archive, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress(FileExistsError):\n",
    "    os.mkdir('data')\n",
    "start_dir = os.path.join('data', 'UCMerced_LandUse', 'Images')\n",
    "if not os.path.isdir(start_dir):\n",
    "    with ZipFile('UCMerced_LandUse.zip') as z:\n",
    "        z.extractall(path='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of all TIFF image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a {number: name} class dictionary for later reference\n",
    "classes = OrderedDict()\n",
    "labels = []\n",
    "filenames = []\n",
    "for index, (root, _, files) in enumerate(os.walk(start_dir, topdown=False)):\n",
    "    if files:\n",
    "        class_name = os.path.basename(root)\n",
    "        classes[index] = class_name\n",
    "        filenames.extend([os.path.join(root, file) for file in files])\n",
    "        labels.extend([index]*len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize all images to common shape; output to \"transformed\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dir = os.path.join('data', 'transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_images(image_files, outdir):\n",
    "    with suppress(FileExistsError):\n",
    "        os.mkdir(outdir)\n",
    "    # Check if directory is empty before processing\n",
    "    if not os.listdir(outdir):\n",
    "        # Pass 1 over images determines smallest image dimension\n",
    "        new_dim = 256\n",
    "        for image_file in image_files:            \n",
    "            with Image.open(image_file) as img:\n",
    "                new_dim = min(new_dim, min(img.size))\n",
    "        \n",
    "        # Pass 2 resizes all images to common dimensions\n",
    "        for image_num, image_file in enumerate(image_files):\n",
    "            with Image.open(image_file) as img:\n",
    "                _, ext = os.path.splitext(image_file)\n",
    "                path = os.path.join(outdir, str(image_num).zfill(4) + ext)\n",
    "                img.resize((new_dim, new_dim)).save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_images(filenames, transformed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pretrained CNN bottleneck feature weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly split images into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_idx = np.arange(len(labels))\n",
    "X_train_idx, X_valid_idx, y_train, y_valid = train_test_split(image_idx, labels, test_size=500, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(image_dir, indexes, batch_size=32):\n",
    "    filenames = os.listdir(image_dir)\n",
    "    images = []\n",
    "    for n, index in enumerate(indexes):\n",
    "        image = imread(os.path.join(image_dir, filenames[index]))\n",
    "        images.append(image/255.0)\n",
    "        if len(images) == batch_size:\n",
    "            X = np.array(images)\n",
    "            images = []\n",
    "            yield X\n",
    "    if images:\n",
    "        yield np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = applications.VGG16(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating bottleneck predictions for bottleneck_features_train.npy\n",
      "Batch 1 (64, 7, 7, 512)\n",
      "Batch 2 (64, 7, 7, 512)\n",
      "Batch 3 (64, 7, 7, 512)\n",
      "Batch 4 (64, 7, 7, 512)\n",
      "Batch 5 (64, 7, 7, 512)\n",
      "Batch 6 (64, 7, 7, 512)\n",
      "Batch 7 (64, 7, 7, 512)\n",
      "Batch 8 (64, 7, 7, 512)\n",
      "Batch 9 (64, 7, 7, 512)\n",
      "Batch 10 (64, 7, 7, 512)\n",
      "Batch 11 (64, 7, 7, 512)\n",
      "Batch 12 (64, 7, 7, 512)\n",
      "Batch 13 (64, 7, 7, 512)\n",
      "Batch 14 (64, 7, 7, 512)\n",
      "Batch 15 (64, 7, 7, 512)\n",
      "Batch 16 (64, 7, 7, 512)\n",
      "Batch 17 (64, 7, 7, 512)\n",
      "Batch 18 (64, 7, 7, 512)\n",
      "Batch 19 (64, 7, 7, 512)\n",
      "Batch 20 (64, 7, 7, 512)\n",
      "Batch 21 (64, 7, 7, 512)\n",
      "Batch 22 (64, 7, 7, 512)\n",
      "Batch 23 (64, 7, 7, 512)\n",
      "Batch 24 (64, 7, 7, 512)\n",
      "Batch 25 (64, 7, 7, 512)\n",
      "Generating bottleneck predictions for bottleneck_features_valid.npy\n",
      "Batch 1 (64, 7, 7, 512)\n",
      "Batch 2 (64, 7, 7, 512)\n",
      "Batch 3 (64, 7, 7, 512)\n",
      "Batch 4 (64, 7, 7, 512)\n",
      "Batch 5 (64, 7, 7, 512)\n",
      "Batch 6 (64, 7, 7, 512)\n",
      "Batch 7 (64, 7, 7, 512)\n",
      "Batch 8 (52, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "predict_jobs = [('bottleneck_features_train.npy', X_train_idx),\n",
    "                ('bottleneck_features_valid.npy', X_valid_idx)]\n",
    "\n",
    "for filename, indexes in predict_jobs:\n",
    "    print(f'Generating bottleneck predictions for {filename}')\n",
    "    pred_batches = []\n",
    "    for n, X in enumerate(image_generator(transformed_dir, indexes, batch_size=64)):\n",
    "        pred = model.predict_on_batch(X)\n",
    "        pred_batches.append(pred)\n",
    "        print('Batch', n+1, pred.shape)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        np.save(f, np.concatenate(pred_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class label vectors to categorical one-hot arrays\n",
    "num_classes = len(classes)\n",
    "Y_train = to_categorical(y_train, num_classes)\n",
    "Y_valid = to_categorical(y_valid, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "\n",
    "    with open('bottleneck_features_train.npy', 'rb') as f:\n",
    "        train_data = np.load(f)\n",
    "    train_labels = Y_train\n",
    "\n",
    "    with open('bottleneck_features_valid.npy', 'rb') as f:\n",
    "        validation_data = np.load(f)\n",
    "    validation_labels = Y_valid\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    top_model_weights_path = 'bottleneck_model.h5'\n",
    "    epochs = 25\n",
    "    batch_size = 64\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "1600/1600 [==============================] - 3s - loss: 2.8852 - acc: 0.2606 - val_loss: 1.3563 - val_acc: 0.6340\n",
      "Epoch 2/25\n",
      "1600/1600 [==============================] - 2s - loss: 1.3577 - acc: 0.5538 - val_loss: 0.8687 - val_acc: 0.7900\n",
      "Epoch 3/25\n",
      "1600/1600 [==============================] - 2s - loss: 0.9219 - acc: 0.7137 - val_loss: 0.6426 - val_acc: 0.8480\n",
      "Epoch 4/25\n",
      "1600/1600 [==============================] - 2s - loss: 0.6901 - acc: 0.7944 - val_loss: 0.5439 - val_acc: 0.8520\n",
      "Epoch 5/25\n",
      "1600/1600 [==============================] - 2s - loss: 0.5459 - acc: 0.8369 - val_loss: 0.4581 - val_acc: 0.8760\n",
      "Epoch 6/25\n",
      "1600/1600 [==============================] - 2s - loss: 0.4880 - acc: 0.8463 - val_loss: 0.4080 - val_acc: 0.8820\n",
      "Epoch 7/25\n",
      "1600/1600 [==============================] - 2s - loss: 0.4019 - acc: 0.8825 - val_loss: 0.4143 - val_acc: 0.8700\n",
      "Epoch 8/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.3259 - acc: 0.9056 - val_loss: 0.3889 - val_acc: 0.8880\n",
      "Epoch 9/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.2826 - acc: 0.9206 - val_loss: 0.3412 - val_acc: 0.9020\n",
      "Epoch 10/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.2647 - acc: 0.9231 - val_loss: 0.3801 - val_acc: 0.8660\n",
      "Epoch 11/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.2355 - acc: 0.9325 - val_loss: 0.3204 - val_acc: 0.8940\n",
      "Epoch 12/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.2325 - acc: 0.9325 - val_loss: 0.3415 - val_acc: 0.8820\n",
      "Epoch 13/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1917 - acc: 0.9400 - val_loss: 0.3432 - val_acc: 0.8780\n",
      "Epoch 14/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1788 - acc: 0.9544 - val_loss: 0.3354 - val_acc: 0.8840\n",
      "Epoch 15/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1520 - acc: 0.9569 - val_loss: 0.3112 - val_acc: 0.8940\n",
      "Epoch 16/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1632 - acc: 0.9537 - val_loss: 0.3377 - val_acc: 0.9040\n",
      "Epoch 17/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1468 - acc: 0.9556 - val_loss: 0.3165 - val_acc: 0.9000\n",
      "Epoch 18/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1470 - acc: 0.9556 - val_loss: 0.3211 - val_acc: 0.8900\n",
      "Epoch 19/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1161 - acc: 0.9675 - val_loss: 0.3141 - val_acc: 0.8880\n",
      "Epoch 20/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1086 - acc: 0.9706 - val_loss: 0.3225 - val_acc: 0.8840\n",
      "Epoch 21/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1235 - acc: 0.9669 - val_loss: 0.3238 - val_acc: 0.8900\n",
      "Epoch 22/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1004 - acc: 0.9731 - val_loss: 0.3055 - val_acc: 0.8920\n",
      "Epoch 23/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1226 - acc: 0.9644 - val_loss: 0.3156 - val_acc: 0.9040\n",
      "Epoch 24/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.1157 - acc: 0.9675 - val_loss: 0.3248 - val_acc: 0.8940\n",
      "Epoch 25/25\n",
      "1600/1600 [==============================] - 3s - loss: 0.0935 - acc: 0.9756 - val_loss: 0.3203 - val_acc: 0.9040\n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
