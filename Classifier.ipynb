{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from contextlib import suppress\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the [UC Merced Land Use dataset](http://vision.ucmerced.edu/datasets/landuse.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image files from the zipped archive, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress(FileExistsError):\n",
    "    os.mkdir('data')\n",
    "source_dir = os.path.join('data', 'UCMerced_LandUse', 'Images')\n",
    "\n",
    "# Download the zipped dataset from http://vision.ucmerced.edu/datasets/landuse.html \n",
    "if not os.path.isdir(source_dir):\n",
    "    with ZipFile('UCMerced_LandUse.zip') as z:\n",
    "        z.extractall(path='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly assign each image to train, validate, or test folder, segregated by class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create image directory hierarchy that looks like this:\n",
    "./data/transformed/\n",
    "                   train/\n",
    "                         agriculture/\n",
    "                         airplane/\n",
    "                         ...\n",
    "                   validate/\n",
    "                         agriculture/\n",
    "                         airplane/\n",
    "                         ...\n",
    "                   test/\n",
    "                         agriculture/\n",
    "                         airplane/\n",
    "                         ...\n",
    "\"\"\"\n",
    "\n",
    "# Collect class names from directory names in './data/UCMerced_LandUse/Images/'\n",
    "class_names = os.listdir(source_dir)    \n",
    "\n",
    "# Create path to image \"flow\" base directory\n",
    "flow_base = os.path.join('data', 'flow')\n",
    "\n",
    "# Create pathnames to train/validate/test subdirectories\n",
    "target_dirs = {target: os.path.join(flow_base, target) for target in ['train', 'validate', 'test']}\n",
    "\n",
    "if not os.path.isdir(flow_base):\n",
    "\n",
    "    # Make new directories\n",
    "    os.mkdir(flow_base)\n",
    "    \n",
    "    for target in ['train', 'validate', 'test']:\n",
    "        target_dir = os.path.join(flow_base, target)\n",
    "        os.mkdir(target_dir)\n",
    "        for class_name in class_names:\n",
    "            class_subdir = os.path.join(target_dir, class_name)\n",
    "            os.mkdir(class_subdir)\n",
    "\n",
    "    # Copy images from ./data/UCMerced_LandUse/Images to ./data/flow/<train, validate, test> directories\n",
    "    \n",
    "    warnings.simplefilter('ignore', UserWarning)  # suppress low-contrast warning from skimage.io.imsave\n",
    "    for root, _, filenames in os.walk(source_dir):\n",
    "        if filenames:\n",
    "            class_name = os.path.basename(root)\n",
    "\n",
    "            # Randomly shuffle filenames\n",
    "            filenames = np.random.permutation(filenames)\n",
    "            for target, count in [('train', 80), ('validate', 10), ('test', 10)]:\n",
    "                target_dir = os.path.join(flow_base, target, class_name)\n",
    "                for filename in filenames[:count]:\n",
    "                    filepath = os.path.join(root, filename)\n",
    "                    image = imread(filepath)\n",
    "                    basename, _ = os.path.splitext(filename)\n",
    "                    # Convert TIF to PNG to work with Keras ImageDataGenerator.flow_from_directory\n",
    "                    target_filename = os.path.join(target_dir, basename + '.png')\n",
    "                    imsave(target_filename, image)\n",
    "            \n",
    "                filenames = filenames[count:]\n",
    "    # Show future warnings during development\n",
    "    warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training set bottleneck features from pretrained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bottleneck_features(model, dataset='train', batch_size=32):\n",
    "    \"\"\"\n",
    "    Extract botteleneck features for the input dataset (train/validate/test)\n",
    "    by predicting on the convolutional portion only of a pretrained model.\n",
    "        \n",
    "    Inputs:\n",
    "        model: Pre-trained deep learning model, excluding fully-connected top model\n",
    "               e.g. applications.VGG16(include_top=False, weights='imagenet')\n",
    "        dataset = string label for dataset image directory ['train', 'validate', 'test']\n",
    "    \n",
    "    Return:\n",
    "        Return bottleneck features as numpy.array\n",
    "    \"\"\"\n",
    "    image_size = (256, 256)\n",
    "    image_data_gen = ImageDataGenerator(rescale=1.0/255)\n",
    "    image_generator = image_data_gen.flow_from_directory(target_dirs[dataset],\n",
    "                                                         batch_size=batch_size,\n",
    "                                                         target_size=image_size,\n",
    "                                                         shuffle=False\n",
    "                                                         )\n",
    "    \n",
    "    print(f'Generating \"{dataset}\" bottleneck predictions')\n",
    "    \n",
    "    image_count = 0\n",
    "    X_batches = []\n",
    "    Y_batches = []\n",
    "    for n, (X, Y) in enumerate(image_generator, start=1):\n",
    "        X_batches.append(model.predict_on_batch(X))\n",
    "        Y_batches.append(Y)\n",
    "        image_count += X.shape[0]\n",
    "        # Must interrupt image_generator\n",
    "        if image_count >= image_generator.n:\n",
    "            break\n",
    "    \n",
    "    X = np.concatenate(X_batches)\n",
    "    Y = np.concatenate(Y_batches)\n",
    "    \n",
    "    print(f'   Features of shape {X.shape} extracted for model \"{model.name}\"')\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a pre-trained model from the Keras.applications module; e.g. Xception, VGG16 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception V1 is a smaller-footprint model with high accuracy\n",
    "pretrained_model = applications.Xception(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract bottleneck features for each dataset: train, validate, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1680 images belonging to 21 classes.\n",
      "Generating \"train\" bottleneck predictions\n",
      "   Features of shape (1680, 8, 8, 2048) extracted for model xception\n",
      "Found 210 images belonging to 21 classes.\n",
      "Generating \"validate\" bottleneck predictions\n",
      "   Features of shape (210, 8, 8, 2048) extracted for model xception\n",
      "Found 210 images belonging to 21 classes.\n",
      "Generating \"test\" bottleneck predictions\n",
      "   Features of shape (210, 8, 8, 2048) extracted for model xception\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(class_names)\n",
    "X, Y = dict(), dict()\n",
    "for dataset in ['train', 'validate', 'test']:\n",
    "    # Extract bottleneck features from pretrained model, predicting on images from \"dataset\" directory\n",
    "    X[dataset], Y[dataset]  = extract_bottleneck_features(pretrained_model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a fully-connected model using bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fully_connected(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a fully-connected model to train or test on UC Merced dataset.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1680 samples, validate on 210 samples\n",
      "Epoch 1/15\n",
      "18s - loss: 7.6135 - acc: 0.4244 - val_loss: 5.1898 - val_acc: 0.6000\n",
      "Epoch 2/15\n",
      "16s - loss: 4.0794 - acc: 0.6470 - val_loss: 1.8674 - val_acc: 0.7810\n",
      "Epoch 3/15\n",
      "17s - loss: 1.6379 - acc: 0.7530 - val_loss: 0.9444 - val_acc: 0.8143\n",
      "Epoch 4/15\n",
      "17s - loss: 0.8143 - acc: 0.7905 - val_loss: 0.6842 - val_acc: 0.8333\n",
      "Epoch 5/15\n",
      "17s - loss: 0.5543 - acc: 0.8446 - val_loss: 0.7035 - val_acc: 0.8476\n",
      "Epoch 6/15\n",
      "16s - loss: 0.5313 - acc: 0.8750 - val_loss: 0.8170 - val_acc: 0.8333\n",
      "Epoch 7/15\n",
      "17s - loss: 0.4102 - acc: 0.8768 - val_loss: 0.6724 - val_acc: 0.8524\n",
      "Epoch 8/15\n",
      "17s - loss: 0.3877 - acc: 0.8982 - val_loss: 0.5629 - val_acc: 0.8762\n",
      "Epoch 9/15\n",
      "17s - loss: 0.3175 - acc: 0.9149 - val_loss: 0.6079 - val_acc: 0.8619\n",
      "Epoch 10/15\n",
      "17s - loss: 0.2748 - acc: 0.9226 - val_loss: 0.6349 - val_acc: 0.8667\n",
      "Epoch 11/15\n",
      "17s - loss: 0.2932 - acc: 0.9173 - val_loss: 0.5461 - val_acc: 0.8905\n",
      "Epoch 12/15\n",
      "17s - loss: 0.2580 - acc: 0.9292 - val_loss: 0.6064 - val_acc: 0.8810\n",
      "Epoch 13/15\n",
      "17s - loss: 0.2451 - acc: 0.9339 - val_loss: 0.5401 - val_acc: 0.8857\n",
      "Epoch 14/15\n",
      "17s - loss: 0.2632 - acc: 0.9244 - val_loss: 0.6199 - val_acc: 0.8857\n",
      "Epoch 15/15\n",
      "17s - loss: 0.2260 - acc: 0.9375 - val_loss: 0.5386 - val_acc: 0.8810\n"
     ]
    }
   ],
   "source": [
    "# Build, compile, and fit the model\n",
    "\n",
    "model = build_fully_connected(input_shape=X['train'].shape[1:], num_classes=num_classes)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X['train'], Y['train'], batch_size=64, epochs=15,\n",
    "          verbose=2, validation_data=(X['validate'], Y['validate'])\n",
    "         )\n",
    "\n",
    "# Save model weights for test dataset predictions\n",
    "fit_model_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Validation accuracy ~ 88%. Validation loss has flattened; stop to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by building the same fully-connected model\n",
    "model = build_fully_connected(input_shape=X['test'].shape[1:], num_classes=num_classes)\n",
    "\n",
    "# Load weights from the model fit on the training data\n",
    "model.set_weights(fit_model_weights)\n",
    "\n",
    "# Predict on the test images\n",
    "y_pred = model.predict_classes(X['test'], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predication accuracy: 0.890\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       0.90      0.90      0.90        10\n",
      "          3       1.00      1.00      1.00        10\n",
      "          4       0.88      0.70      0.78        10\n",
      "          5       1.00      1.00      1.00        10\n",
      "          6       0.88      0.70      0.78        10\n",
      "          7       1.00      0.90      0.95        10\n",
      "          8       1.00      1.00      1.00        10\n",
      "          9       0.91      1.00      0.95        10\n",
      "         10       1.00      1.00      1.00        10\n",
      "         11       0.53      0.90      0.67        10\n",
      "         12       0.70      0.70      0.70        10\n",
      "         13       0.75      0.90      0.82        10\n",
      "         14       0.90      0.90      0.90        10\n",
      "         15       1.00      0.90      0.95        10\n",
      "         16       0.91      1.00      0.95        10\n",
      "         17       0.91      1.00      0.95        10\n",
      "         18       1.00      0.80      0.89        10\n",
      "         19       1.00      1.00      1.00        10\n",
      "         20       0.71      0.50      0.59        10\n",
      "\n",
      "avg / total       0.90      0.89      0.89       210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = np.nonzero(Y['test'])[1]\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model predication accuracy: {accuracy:.3f}')\n",
    "print(f'\\nClassification report:\\n {classification_report(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test accuracy ~ 89% vs. ~ 88% validation accuracy. Impressive for such a small image dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
