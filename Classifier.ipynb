{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from contextlib import suppress\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.external.tifffile import imread\n",
    "\n",
    "from keras import applications\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the [UC Merced Land Use dataset](http://vision.ucmerced.edu/datasets/landuse.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image files from the zipped archive, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress(FileExistsError):\n",
    "    os.mkdir('data')\n",
    "source_dir = os.path.join('data', 'UCMerced_LandUse', 'Images')\n",
    "if not os.path.isdir(source_dir):\n",
    "    with ZipFile('UCMerced_LandUse.zip') as z:\n",
    "        z.extractall(path='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:  I discovered that not all images files are 256x256 pixels as claimed at the UC Merced Land Use Dataset site. To work with the Keras models, all images must have the same dimensions, so I resize them to a common shape below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_filepaths(start_dir):\n",
    "    \"\"\"\n",
    "    Helper function to walk a directory structure, collecting\n",
    "    file pathnames for all TIFF images.\n",
    "    \n",
    "    Input:\n",
    "        start_dir: directory where walking starts\n",
    "    \n",
    "    Returns:\n",
    "        List of TIFF file pathnames\n",
    "    \"\"\"\n",
    "    return [os.path.join(root, file) for root, _, files in os.walk(start_dir)\n",
    "                                          for file in files\n",
    "                                              if file.endswith('.tif')\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_name_from_path(filepath):\n",
    "    \"\"\"\n",
    "    Helper function to extract and return an image's class name\n",
    "    from the name of the directory in which the image is stored.\n",
    "    \"\"\"\n",
    "    head, _ = os.path.split(filepath)\n",
    "    _, class_name = os.path.split(head)\n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform each image to a common shape; place in train or validate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = os.path.join('data', 'transformed')\n",
    "train_dir = os.path.join(out_dir, 'train')\n",
    "validate_dir = os.path.join(out_dir, 'validate')\n",
    "\n",
    "# Delete the existing \"transformed\" directory (and all subdirectories and files)\n",
    "with suppress(FileNotFoundError):\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "# Make new, empty directories\n",
    "os.mkdir(out_dir)\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(validate_dir)\n",
    "\n",
    "# Create train and validate label lists\n",
    "train_labels = []\n",
    "validate_labels = []\n",
    "\n",
    "# Get a list of the source image file pathnames from \"data/UCMerced_LandUse/Images\"\n",
    "filepaths = get_image_filepaths(source_dir)\n",
    "\n",
    "# Randomly shuffle the image file pathnames (see for reproducibility)\n",
    "np.random.seed(8)\n",
    "np.random.shuffle(filepaths)\n",
    "\n",
    "# Pass 1:\n",
    "# 1) Collect image classes {name: number} in the \"class_num_by_class_name\" dictionary\n",
    "# 2) Determine the smallest image dimension\n",
    "\n",
    "class_num_by_class_name = dict()\n",
    "class_num_by_filepath = dict()\n",
    "\n",
    "new_dim = 256\n",
    "for filepath in filepaths:\n",
    "    # Derive image file's class name from the file pathname\n",
    "    class_name = class_name_from_path(filepath)\n",
    "\n",
    "    # Add (potentially) new class to \"class_num_by_class_name\" dictionary\n",
    "    class_num_by_class_name.setdefault(class_name, len(class_num_by_class_name))\n",
    "    \n",
    "    # Store class number for future reference\n",
    "    class_num_by_filepath[filepath] = class_num_by_class_name[class_name]\n",
    "\n",
    "    # Find the minimum height or width dimension of all images\n",
    "    with Image.open(filepath) as img:\n",
    "        new_dim = min(new_dim, min(img.size))\n",
    "\n",
    "# Pass 2:\n",
    "# 1) Randomly split (e.g. 80/20) images between the train and validate directories\n",
    "# 2) Resize all images to a common (new_dim, new_dim) size\n",
    "# 3) Save class label information for each image\n",
    "\n",
    "for image_num, filepath in enumerate(filepaths):\n",
    "    with Image.open(filepath) as img:\n",
    "        # Separate images between train/validate directories \n",
    "        target = train_dir if np.random.rand() < 0.80 else validate_dir\n",
    "        \n",
    "        # Name images in numbered format nnnn.tif\n",
    "        _, ext = os.path.splitext(filepath)\n",
    "        path = os.path.join(target, str(image_num).zfill(4) + ext)\n",
    "\n",
    "        # Resize image to common shape and save to target directory\n",
    "        img.resize((new_dim, new_dim)).save(path)\n",
    "        \n",
    "        # Capture class label to \"train_labels\" or \"validate_labels\" list\n",
    "        class_num = class_num_by_filepath[filepath]\n",
    "        train_labels.append(class_num) if target == train_dir else validate_labels.append(class_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pretrained CNN bottleneck feature weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(image_dir, batch_size=64):\n",
    "    images = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        image = imread(os.path.join(image_dir, filename))\n",
    "        # Min-Max scale the image to range 0.0 - 1.0\n",
    "        images.append(image/255.0)\n",
    "        if len(images) == batch_size:\n",
    "            X = np.array(images)\n",
    "            images = []\n",
    "            yield X\n",
    "    if images:\n",
    "        yield np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "model = applications.Xception(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training bottleneck predictions\n",
      "Batch 27, shape (39, 8, 8, 2048)\n",
      "Feature weights saved to bn_training.npy\n",
      "Generating validation bottleneck predictions\n",
      "Batch 7, shape (13, 8, 8, 2048)\n",
      "Feature weights saved to bn_validation.npy\n"
     ]
    }
   ],
   "source": [
    "bn_features = dict()\n",
    "for job_name, image_dir in [('training', train_dir), ('validation', validate_dir)]:\n",
    "    print(f'Generating {job_name} bottleneck predictions')\n",
    "    pred_batches = []\n",
    "    for n, X in enumerate(image_generator(image_dir, batch_size=64)):\n",
    "        pred = model.predict_on_batch(X)\n",
    "        pred_batches.append(pred)\n",
    "        print(f'Batch {n+1}, shape {pred.shape}', end='\\r')\n",
    "\n",
    "    # Save weights to bn_features dictionary\n",
    "    bn_features[job_name] = np.concatenate(pred_batches)\n",
    "    filename = 'bn_' + job_name + '.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        print(f'\\nFeature weights saved to {filename}')\n",
    "        np.save(f, bn_features[job_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new fully-connected model, using bottleneck features as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class label vectors to categorical one-hot arrays\n",
    "num_classes = len(class_num_by_class_name)\n",
    "Y_train = to_categorical(train_labels, num_classes)\n",
    "Y_valid = to_categorical(validate_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "\n",
    "    with open('bn_training.npy', 'rb') as f:\n",
    "        train_data = np.load(f)\n",
    "    train_labels = Y_train\n",
    "\n",
    "    with open('bn_validation.npy', 'rb') as f:\n",
    "        validation_data = np.load(f)\n",
    "    validation_labels = Y_valid\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    top_model_weights_path = 'bottleneck_model.h5'\n",
    "    epochs = 25\n",
    "    batch_size = 64\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1703 samples, validate on 397 samples\n",
      "Epoch 1/25\n",
      "1703/1703 [==============================] - 16s - loss: 7.5076 - acc: 0.4110 - val_loss: 2.8893 - val_acc: 0.7481\n",
      "Epoch 2/25\n",
      "1703/1703 [==============================] - 15s - loss: 3.5402 - acc: 0.7076 - val_loss: 1.4458 - val_acc: 0.8237\n",
      "Epoch 3/25\n",
      "1703/1703 [==============================] - 15s - loss: 1.9479 - acc: 0.8056 - val_loss: 1.2017 - val_acc: 0.8413\n",
      "Epoch 4/25\n",
      "1703/1703 [==============================] - 15s - loss: 1.0773 - acc: 0.8262 - val_loss: 0.4649 - val_acc: 0.8715\n",
      "Epoch 5/25\n",
      "1703/1703 [==============================] - 16s - loss: 0.6295 - acc: 0.8456 - val_loss: 0.4448 - val_acc: 0.8816\n",
      "Epoch 6/25\n",
      "1703/1703 [==============================] - 16s - loss: 0.4781 - acc: 0.8814 - val_loss: 0.4062 - val_acc: 0.8917\n",
      "Epoch 7/25\n",
      "1703/1703 [==============================] - 16s - loss: 0.3148 - acc: 0.9143 - val_loss: 0.4687 - val_acc: 0.8892\n",
      "Epoch 8/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.2953 - acc: 0.9184 - val_loss: 0.3323 - val_acc: 0.9219\n",
      "Epoch 9/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.2927 - acc: 0.9284 - val_loss: 0.4280 - val_acc: 0.8967\n",
      "Epoch 10/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.2428 - acc: 0.9430 - val_loss: 0.4686 - val_acc: 0.9169\n",
      "Epoch 11/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.3097 - acc: 0.9307 - val_loss: 0.5418 - val_acc: 0.9018\n",
      "Epoch 12/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.2380 - acc: 0.9378 - val_loss: 0.4444 - val_acc: 0.9169\n",
      "Epoch 13/25\n",
      "1703/1703 [==============================] - 18s - loss: 0.1795 - acc: 0.9489 - val_loss: 0.5943 - val_acc: 0.8942\n",
      "Epoch 14/25\n",
      "1703/1703 [==============================] - 18s - loss: 0.1732 - acc: 0.9536 - val_loss: 0.4009 - val_acc: 0.9370\n",
      "Epoch 15/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.1524 - acc: 0.9583 - val_loss: 0.5052 - val_acc: 0.9093\n",
      "Epoch 16/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.1812 - acc: 0.9530 - val_loss: 0.5052 - val_acc: 0.9068\n",
      "Epoch 17/25\n",
      "1703/1703 [==============================] - 18s - loss: 0.1926 - acc: 0.9554 - val_loss: 0.5611 - val_acc: 0.9219\n",
      "Epoch 18/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.1867 - acc: 0.9589 - val_loss: 0.4512 - val_acc: 0.9295\n",
      "Epoch 19/25\n",
      "1703/1703 [==============================] - 18s - loss: 0.1865 - acc: 0.9524 - val_loss: 0.4051 - val_acc: 0.9144\n",
      "Epoch 20/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.1398 - acc: 0.9677 - val_loss: 0.4788 - val_acc: 0.9169\n",
      "Epoch 21/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.1442 - acc: 0.9595 - val_loss: 0.6006 - val_acc: 0.9118\n",
      "Epoch 22/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.1690 - acc: 0.9612 - val_loss: 0.4039 - val_acc: 0.9270\n",
      "Epoch 23/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.1929 - acc: 0.9583 - val_loss: 0.5012 - val_acc: 0.9345\n",
      "Epoch 24/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.1707 - acc: 0.9595 - val_loss: 0.4844 - val_acc: 0.9219\n",
      "Epoch 25/25\n",
      "1703/1703 [==============================] - 17s - loss: 0.1399 - acc: 0.9683 - val_loss: 0.4167 - val_acc: 0.9295\n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
