{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from contextlib import suppress\n",
    "import numpy as np\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.external.tifffile import imread\n",
    "\n",
    "from keras import applications\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the [UC Merced Land Use dataset](http://vision.ucmerced.edu/datasets/landuse.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image files from the zipped archive, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress(FileExistsError):\n",
    "    os.mkdir('data')\n",
    "source_dir = os.path.join('data', 'UCMerced_LandUse', 'Images')\n",
    "\n",
    "# Download the zipped dataset from http://vision.ucmerced.edu/datasets/landuse.html \n",
    "if not os.path.isdir(source_dir):\n",
    "    with ZipFile('UCMerced_LandUse.zip') as z:\n",
    "        z.extractall(path='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:  I discovered that not all images files are 256x256 pixels as claimed at the UC Merced Land Use Dataset site. To work with the Keras models, all images must have the same dimensions, so I resize them to a common shape below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_filepaths(start_dir):\n",
    "    \"\"\"\n",
    "    Helper function to walk a directory structure, collecting\n",
    "    file pathnames for all TIFF images.\n",
    "    \n",
    "    Input:\n",
    "        start_dir: directory where walking starts\n",
    "    \n",
    "    Returns:\n",
    "        List of TIFF file pathnames\n",
    "    \"\"\"\n",
    "    return [os.path.join(root, file) for root, _, files in os.walk(start_dir)\n",
    "                                          for file in files\n",
    "                                              if file.endswith('.tif')\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_name_from_path(filepath):\n",
    "    \"\"\"\n",
    "    Extract and return an image's class name from the name\n",
    "    of the directory in which the image is stored.\n",
    "    \"\"\"\n",
    "    head, _ = os.path.split(filepath)\n",
    "    _, class_name = os.path.split(head)\n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_target():\n",
    "    \"\"\"\n",
    "    Randomize splits between train, validate, and test directories.\n",
    "    \"\"\"\n",
    "    # Use an 80% training data split\n",
    "    train_split = 0.8\n",
    "    if np.random.rand() < train_split:\n",
    "        return 'train'\n",
    "    # 50/50 split for validation and test data\n",
    "    return 'validate' if np.random.rand() < 0.5 else 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_filename(image_num, class_num):\n",
    "    \"\"\"\n",
    "    Return a TIF file name string in the format <image_num>_<class_num>.tif\n",
    "    \"\"\"\n",
    "    return str(image_num).zfill(4) + '_' + str(class_num).zfill(2) + '.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classnum_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract and return the int class number from an image filename\n",
    "    formatted by make_image_filename().\n",
    "    \"\"\"\n",
    "    return int(filename.split('.')[0][-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(image_dir, batch_size=64):\n",
    "    \"\"\"\n",
    "    Generate a stream of min-max scaled images from image_dir,\n",
    "    returned as numpy.array of sample size batch_size. \n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        image = imread(os.path.join(image_dir, filename))\n",
    "        # Min-Max scale the image to range 0.0 - 1.0\n",
    "        images.append(image/255.0)\n",
    "        if len(images) == batch_size:\n",
    "            X = np.array(images)\n",
    "            images = []\n",
    "            yield X\n",
    "    if images:\n",
    "        yield np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformed-image target directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create image directory hierarchy that looks like this:\n",
    "data/transformed/\n",
    "                 train/\n",
    "                       ...\n",
    "                 validate/\n",
    "                       ...\n",
    "                 test/\n",
    "                       ...\n",
    "\"\"\"\n",
    "target_base = os.path.join('data', 'transformed')\n",
    "target_dirs = {target: os.path.join(target_base, target) for target in ['train', 'validate', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform each image to a common shape; place in train, validate, or test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = defaultdict(list)\n",
    "\n",
    "# Assume if directory \"transformed\" exists, it contains all the processed images.\n",
    "if not os.path.isdir(target_base):\n",
    "    # Make new directories\n",
    "    os.mkdir(target_base)\n",
    "    for target in target_dirs:\n",
    "        os.mkdir(target_dirs[target])\n",
    "    \n",
    "    # Get a list of the source image file pathnames from \"data/UCMerced_LandUse/Images\"\n",
    "    filepaths = get_image_filepaths(source_dir)\n",
    "\n",
    "    # Randomly shuffle the image file pathnames \n",
    "    np.random.shuffle(filepaths)\n",
    "\n",
    "    # Pass 1:\n",
    "    # 1) Collect image classes {name: number} in the \"class_num_by_class_name\" dictionary\n",
    "    # 2) Determine the smallest image dimension\n",
    "\n",
    "    class_num_by_class_name = dict()\n",
    "    class_num_by_filepath = dict()\n",
    "\n",
    "    new_dim = 256\n",
    "    for filepath in filepaths:\n",
    "        # Derive image file's class name from the file pathname\n",
    "        class_name = class_name_from_path(filepath)\n",
    "\n",
    "        # Add (potentially) new class to \"class_num_by_class_name\" dictionary\n",
    "        class_num_by_class_name.setdefault(class_name, len(class_num_by_class_name))\n",
    "\n",
    "        # Store class number for future reference\n",
    "        class_num_by_filepath[filepath] = class_num_by_class_name[class_name]\n",
    "\n",
    "        # Find the minimum height or width dimension of all images\n",
    "        with Image.open(filepath) as img:\n",
    "            new_dim = min(new_dim, min(img.size))\n",
    "\n",
    "    # Pass 2:\n",
    "    # 1) Randomly split images between the train, validate, and test directories\n",
    "    # 2) Resize all images to a common (new_dim, new_dim) size\n",
    "    # 3) Save class label information for each image\n",
    "\n",
    "    for image_num, filepath in enumerate(filepaths):\n",
    "        with Image.open(filepath) as img:\n",
    "            # Separate images between train/validate/test directories \n",
    "            target = choose_target()\n",
    "\n",
    "            # Capture class label number\n",
    "            class_num = class_num_by_filepath[filepath]\n",
    "            labels[target].append(class_num)\n",
    "            \n",
    "            # Name images in numbered format <image#>_<class#>.tif\n",
    "            path = os.path.join(target_dirs[target], make_image_filename(image_num, class_num))\n",
    "\n",
    "            # Resize image to common shape and save to target directory\n",
    "            img.resize((new_dim, new_dim)).save(path)\n",
    "\n",
    "# else if directory \"transformed\" exists\n",
    "else:\n",
    "    # get labels\n",
    "    for target_name, target_dir in target_dirs.items():\n",
    "        for filename in os.listdir(target_dir):\n",
    "            labels[target_name].append(classnum_from_filename(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get training set bottleneck features from pretrained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bottleneck_features(model, dataset='train', batch_size=64):\n",
    "    \"\"\"\n",
    "    Extract botteleneck features for the input dataset (train/validate/test)\n",
    "    by predicting on the convolutional portion only of a pretrained model.\n",
    "        \n",
    "    Inputs:\n",
    "        model: Pre-trained deep learning model, excluding fully-connected top model\n",
    "               e.g. applications.VGG16(include_top=False, weights='imagenet')\n",
    "        dataset = string label for dataset image directory ['train', 'validate', 'test']\n",
    "    \n",
    "    Return:\n",
    "        Return bottleneck features as numpy.array\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Generating \"{dataset}\" bottleneck predictions')\n",
    "    dir_ = target_dirs[dataset]\n",
    "    pred_batches = [model.predict_on_batch(X) for X in image_generator(dir_, batch_size=batch_size)]\n",
    "    \n",
    "    # Concatenate predictions list to numpy.array\n",
    "    bn_features = np.concatenate(pred_batches)\n",
    "    print(f'   Features of shape {bn_features.shape} extracted for model {model.name}')\n",
    "    \n",
    "    return bn_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a pre-trained model from the Keras.applications module; e.g. Xception, VGG16 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception V1 is a smaller-footprint model with high accuracy\n",
    "pretrained_model = applications.Xception(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract bottleneck features for each dataset: train, validate, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating \"train\" bottleneck predictions\n",
      "   Features of shape (1675, 8, 8, 2048) extracted for model xception\n",
      "Generating \"validate\" bottleneck predictions\n",
      "   Features of shape (216, 8, 8, 2048) extracted for model xception\n",
      "Generating \"test\" bottleneck predictions\n",
      "   Features of shape (209, 8, 8, 2048) extracted for model xception\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(np.unique(labels['train']))\n",
    "X, Y = dict(), dict()\n",
    "for dataset in ['train', 'validate', 'test']:\n",
    "    # Extract bottleneck features from pretrained model, predicting on images from \"dataset\" directory\n",
    "    X[dataset] = extract_bottleneck_features(pretrained_model, dataset)\n",
    "    # Convert class label vectors to categorical one-hot arrays\n",
    "    Y[dataset] = to_categorical(labels[dataset], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a fully-connected model using bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fully_connected(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a fully-connected model to train or test on UC Merced dataset.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1675 samples, validate on 216 samples\n",
      "Epoch 1/10\n",
      "17s - loss: 6.2914 - acc: 0.4764 - val_loss: 3.3246 - val_acc: 0.7361\n",
      "Epoch 2/10\n",
      "17s - loss: 3.4602 - acc: 0.7069 - val_loss: 1.6056 - val_acc: 0.8056\n",
      "Epoch 3/10\n",
      "17s - loss: 1.6102 - acc: 0.7767 - val_loss: 0.7901 - val_acc: 0.8148\n",
      "Epoch 4/10\n",
      "17s - loss: 0.7913 - acc: 0.8066 - val_loss: 0.6921 - val_acc: 0.8657\n",
      "Epoch 5/10\n",
      "17s - loss: 0.5441 - acc: 0.8555 - val_loss: 0.5959 - val_acc: 0.8657\n",
      "Epoch 6/10\n",
      "17s - loss: 0.4473 - acc: 0.8878 - val_loss: 0.6793 - val_acc: 0.8380\n",
      "Epoch 7/10\n",
      "17s - loss: 0.3845 - acc: 0.9045 - val_loss: 0.4596 - val_acc: 0.8796\n",
      "Epoch 8/10\n",
      "17s - loss: 0.2460 - acc: 0.9284 - val_loss: 0.4562 - val_acc: 0.8889\n",
      "Epoch 9/10\n",
      "17s - loss: 0.2659 - acc: 0.9319 - val_loss: 0.5339 - val_acc: 0.8889\n",
      "Epoch 10/10\n",
      "17s - loss: 0.3110 - acc: 0.9260 - val_loss: 0.5375 - val_acc: 0.9074\n"
     ]
    }
   ],
   "source": [
    "# Build, compile, and fit the model\n",
    "\n",
    "model = build_fully_connected(input_shape=X['train'].shape[1:], num_classes=num_classes)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X['train'], Y['train'], batch_size=64, epochs=10,\n",
    "          verbose=2, validation_data=(X['validate'], Y['validate'])\n",
    "         )\n",
    "\n",
    "# Save model weights for test dataset predictions\n",
    "fit_model_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Validation accuracy ~90% !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by building the same fully-connected model\n",
    "model = build_fully_connected(input_shape=X['test'].shape[1:], num_classes=num_classes)\n",
    "\n",
    "# Load weights from the model fit on the training data\n",
    "model.set_weights(fit_model_weights)\n",
    "\n",
    "# Predict on the test images\n",
    "y_pred = model.predict_classes(X['test'], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predication accuracy: 0.866\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.60      0.72        15\n",
      "          1       1.00      1.00      1.00         9\n",
      "          2       1.00      0.75      0.86         8\n",
      "          3       0.87      0.87      0.87        15\n",
      "          4       1.00      0.38      0.55         8\n",
      "          5       1.00      1.00      1.00        11\n",
      "          6       0.77      1.00      0.87        10\n",
      "          7       1.00      0.70      0.82        10\n",
      "          8       0.38      1.00      0.55         3\n",
      "          9       0.88      1.00      0.93        14\n",
      "         10       0.36      0.80      0.50         5\n",
      "         11       1.00      0.88      0.93         8\n",
      "         12       1.00      0.92      0.96        12\n",
      "         13       1.00      1.00      1.00        10\n",
      "         14       1.00      1.00      1.00         7\n",
      "         15       1.00      0.89      0.94         9\n",
      "         16       1.00      1.00      1.00        14\n",
      "         17       0.60      0.60      0.60        10\n",
      "         18       1.00      0.86      0.92         7\n",
      "         19       0.92      1.00      0.96        11\n",
      "         20       0.80      0.92      0.86        13\n",
      "\n",
      "avg / total       0.90      0.87      0.87       209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels['test'], y_pred)\n",
    "print(f'Model predication accuracy: {accuracy:.3f}')\n",
    "print(f'\\nClassification report:\\n {classification_report(labels[\"test\"], y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test accuracy ~87% vs. 90% validation accuracy suggests overfitting in the model and room for improvement. Nonetheless, 87% test accuracy is impressive for a small dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
