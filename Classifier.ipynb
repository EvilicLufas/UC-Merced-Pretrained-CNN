{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from contextlib import suppress\n",
    "import numpy as np\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.external.tifffile import imread\n",
    "\n",
    "from keras import applications\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the [UC Merced Land Use dataset](http://vision.ucmerced.edu/datasets/landuse.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract image files from the zipped archive, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress(FileExistsError):\n",
    "    os.mkdir('data')\n",
    "source_dir = os.path.join('data', 'UCMerced_LandUse', 'Images')\n",
    "if not os.path.isdir(source_dir):\n",
    "    with ZipFile('UCMerced_LandUse.zip') as z:\n",
    "        z.extractall(path='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:  I discovered that not all images files are 256x256 pixels as claimed at the UC Merced Land Use Dataset site. To work with the Keras models, all images must have the same dimensions, so I resize them to a common shape below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_filepaths(start_dir):\n",
    "    \"\"\"\n",
    "    Helper function to walk a directory structure, collecting\n",
    "    file pathnames for all TIFF images.\n",
    "    \n",
    "    Input:\n",
    "        start_dir: directory where walking starts\n",
    "    \n",
    "    Returns:\n",
    "        List of TIFF file pathnames\n",
    "    \"\"\"\n",
    "    return [os.path.join(root, file) for root, _, files in os.walk(start_dir)\n",
    "                                          for file in files\n",
    "                                              if file.endswith('.tif')\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_name_from_path(filepath):\n",
    "    \"\"\"\n",
    "    Helper function to extract and return an image's class name\n",
    "    from the name of the directory in which the image is stored.\n",
    "    \"\"\"\n",
    "    head, _ = os.path.split(filepath)\n",
    "    _, class_name = os.path.split(head)\n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_target():\n",
    "    \"\"\"\n",
    "    Helper function to randomize splits between train/validate/test directories.\n",
    "    \"\"\"\n",
    "    # Use an 80% training data split\n",
    "    train_split = 0.8\n",
    "    if np.random.rand() < train_split:\n",
    "        return 'train'\n",
    "    # 50/50 split for validation and test data\n",
    "    return 'validate' if np.random.rand() < 0.5 else 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_filename(image_num, class_num):\n",
    "    return str(image_num).zfill(4) + '_' + str(class_num).zfill(2) + '.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classnum_from_filename(filename):\n",
    "    # Extract class number from last 2 characters before filename extension\n",
    "    return int(filename.split('.')[0][-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformed-image target directory paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create image directory hierarchy that looks like this:\n",
    "data/transformed/\n",
    "                 train/\n",
    "                       ...\n",
    "                 validate/\n",
    "                       ...\n",
    "                 test/\n",
    "                       ...\n",
    "\"\"\"\n",
    "out_dir = os.path.join('data', 'transformed')\n",
    "target_dirs = {target: os.path.join(out_dir, target) for target in ['train', 'validate', 'test']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform each image to a common shape; place in train or validate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = defaultdict(list)\n",
    "\n",
    "# Assume if directory \"transformed\" exists, it contains all the processed images.\n",
    "if not os.path.isdir(out_dir):\n",
    "    # Make new directories\n",
    "    os.mkdir(out_dir)\n",
    "    for target in target_dirs:\n",
    "        os.mkdir(target_dirs[target])\n",
    "    \n",
    "    # Get a list of the source image file pathnames from \"data/UCMerced_LandUse/Images\"\n",
    "    filepaths = get_image_filepaths(source_dir)\n",
    "\n",
    "    # Randomly shuffle the image file pathnames \n",
    "    np.random.shuffle(filepaths)\n",
    "\n",
    "    # Pass 1:\n",
    "    # 1) Collect image classes {name: number} in the \"class_num_by_class_name\" dictionary\n",
    "    # 2) Determine the smallest image dimension\n",
    "\n",
    "    class_num_by_class_name = dict()\n",
    "    class_num_by_filepath = dict()\n",
    "\n",
    "    new_dim = 256\n",
    "    for filepath in filepaths:\n",
    "        # Derive image file's class name from the file pathname\n",
    "        class_name = class_name_from_path(filepath)\n",
    "\n",
    "        # Add (potentially) new class to \"class_num_by_class_name\" dictionary\n",
    "        class_num_by_class_name.setdefault(class_name, len(class_num_by_class_name))\n",
    "\n",
    "        # Store class number for future reference\n",
    "        class_num_by_filepath[filepath] = class_num_by_class_name[class_name]\n",
    "\n",
    "        # Find the minimum height or width dimension of all images\n",
    "        with Image.open(filepath) as img:\n",
    "            new_dim = min(new_dim, min(img.size))\n",
    "\n",
    "    # Pass 2:\n",
    "    # 1) Randomly split images between the train, validate, and test directories\n",
    "    # 2) Resize all images to a common (new_dim, new_dim) size\n",
    "    # 3) Save class label information for each image\n",
    "\n",
    "    for image_num, filepath in enumerate(filepaths):\n",
    "        with Image.open(filepath) as img:\n",
    "            # Separate images between train/validate/test directories \n",
    "            target = choose_target()\n",
    "\n",
    "            # Capture class label number\n",
    "            class_num = class_num_by_filepath[filepath]\n",
    "            labels[target].append(class_num)\n",
    "            \n",
    "            # Name images in numbered format <image#>_<class#>.tif\n",
    "            path = os.path.join(target_dirs[target], make_image_filename(image_num, class_num))\n",
    "\n",
    "            # Resize image to common shape and save to target directory\n",
    "            img.resize((new_dim, new_dim)).save(path)\n",
    "\n",
    "# else if directory \"transformed\" exists\n",
    "else:\n",
    "    # get labels\n",
    "    for target_name, target_dir in target_dirs.items():\n",
    "        for filename in os.listdir(target_dir):\n",
    "            labels[target_name].append(classnum_from_filename(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pretrained CNN bottleneck feature weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(image_dir, batch_size=64):\n",
    "    images = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        image = imread(os.path.join(image_dir, filename))\n",
    "        # Min-Max scale the image to range 0.0 - 1.0\n",
    "        images.append(image/255.0)\n",
    "        if len(images) == batch_size:\n",
    "            X = np.array(images)\n",
    "            images = []\n",
    "            yield X\n",
    "    if images:\n",
    "        yield np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "model = applications.Xception(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training bottleneck predictions:\n",
      "\tBatch 27, shape (11, 8, 8, 2048)\n",
      "\tFeature weights saved to bn_training.npy\n",
      "\n",
      "Generating validation bottleneck predictions:\n",
      "\tBatch 4, shape (24, 8, 8, 2048)\n",
      "\tFeature weights saved to bn_validation.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_dirs = {target: os.path.join(out_dir, target) for target in ['train', 'validate', 'test']}\n",
    "bn_features = dict()\n",
    "predict_task = [('training', target_dirs['train']), ('validation', target_dirs['validate'])]\n",
    "for task, image_dir in predict_task:\n",
    "    print(f'Generating {task} bottleneck predictions:')\n",
    "    pred_batches = []\n",
    "    for n, X in enumerate(image_generator(image_dir, batch_size=64)):\n",
    "        pred = model.predict_on_batch(X)\n",
    "        pred_batches.append(pred)\n",
    "        print(f'\\tBatch {n+1}, shape {pred.shape}', end='\\r')\n",
    "\n",
    "    # Save weights to bn_features dictionary\n",
    "    bn_features[task] = np.concatenate(pred_batches)\n",
    "    filename = 'bn_' + task + '.npy'\n",
    "    with open(filename, 'wb') as f:\n",
    "        print(f'\\n\\tFeature weights saved to {filename}\\n')\n",
    "        np.save(f, bn_features[task])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new fully-connected model, using bottleneck features as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class label vectors to categorical one-hot arrays\n",
    "num_classes = len(np.unique(labels['train']))\n",
    "Y_train = to_categorical(labels['train'], num_classes)\n",
    "Y_valid = to_categorical(labels['validate'], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "\n",
    "    with open('bn_training.npy', 'rb') as f:\n",
    "        train_data = np.load(f)\n",
    "    train_labels = Y_train\n",
    "\n",
    "    with open('bn_validation.npy', 'rb') as f:\n",
    "        validation_data = np.load(f)\n",
    "    validation_labels = Y_valid\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    top_model_weights_path = 'bottleneck_model.h5'\n",
    "    epochs = 10\n",
    "    batch_size = 64\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1675 samples, validate on 216 samples\n",
      "Epoch 1/10\n",
      "1675/1675 [==============================] - 18s - loss: 6.2914 - acc: 0.4764 - val_loss: 3.3246 - val_acc: 0.7361\n",
      "Epoch 2/10\n",
      "1675/1675 [==============================] - 18s - loss: 3.4602 - acc: 0.7069 - val_loss: 1.6048 - val_acc: 0.8056\n",
      "Epoch 3/10\n",
      "1675/1675 [==============================] - 18s - loss: 1.6210 - acc: 0.7749 - val_loss: 0.7899 - val_acc: 0.8148\n",
      "Epoch 4/10\n",
      "1675/1675 [==============================] - 18s - loss: 0.7407 - acc: 0.8131 - val_loss: 0.8032 - val_acc: 0.8519\n",
      "Epoch 5/10\n",
      "1675/1675 [==============================] - 18s - loss: 0.5402 - acc: 0.8693 - val_loss: 0.5934 - val_acc: 0.8565\n",
      "Epoch 6/10\n",
      "1675/1675 [==============================] - 18s - loss: 0.4234 - acc: 0.8878 - val_loss: 0.5417 - val_acc: 0.8750\n",
      "Epoch 7/10\n",
      "1675/1675 [==============================] - 18s - loss: 0.3199 - acc: 0.9182 - val_loss: 0.4319 - val_acc: 0.9074\n",
      "Epoch 8/10\n",
      "1675/1675 [==============================] - 18s - loss: 0.2953 - acc: 0.9146 - val_loss: 0.4556 - val_acc: 0.9074\n",
      "Epoch 9/10\n",
      "1675/1675 [==============================] - 18s - loss: 0.2500 - acc: 0.9361 - val_loss: 0.4742 - val_acc: 0.8981\n",
      "Epoch 10/10\n",
      "1675/1675 [==============================] - 18s - loss: 0.2042 - acc: 0.9361 - val_loss: 0.5804 - val_acc: 0.8981\n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The fully-connected model achieves ~ 90% validation accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Next up, testing this model on the images set aside in the test directory (coming soon) ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
